{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79856330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper utilities\n",
    "\"\"\"\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def load_png(path) -> Image:\n",
    "    \"\"\"\n",
    "    Loads an PNG image replacing the alpha background with a neutral gray.\n",
    "    Result is returned in RGB\n",
    "    \"\"\"\n",
    "    image = Image.open(path).convert('RGBA')\n",
    "    bg = Image.new('RGB', image.size, (128, 128, 128))\n",
    "    image_rgb = Image.alpha_composite(bg.convert('RGBA'), image).convert('RGB')\n",
    "    return image_rgb\n",
    "\n",
    "def image_to_tensor(image: Image) -> torch.Tensor:\n",
    "    \"\"\"Converts an RGB image to a Tensor\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda t: 2.0 * t - 1.0)\n",
    "    ])\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c1c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ReferenceNet\n",
    "\"\"\"\n",
    "from torch import nn\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, features, out_channels, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(features, hidden_dims),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dims, out_channels*2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, features):\n",
    "        features = self.proj(features)\n",
    "        gamma, beta = features.chunk(2, dim=-1)\n",
    "\n",
    "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        return gamma * x + beta\n",
    "\n",
    "class ReferenceNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, out_width, out_height,\n",
    "                 unet_channels, clip_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_film = FiLM(clip_features, in_channels, 64)\n",
    "\n",
    "        self.unet_film = FiLM(out_channels*out_width*out_height, unet_channels, 128)\n",
    "        \n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,\n",
    "                      kernel_size=3, stride=2, padding=1), # Stride = 2 will half the resolution\n",
    "            nn.GroupNorm(4, out_channels), # channels must be divisible by groups\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, unet, latent, clip):\n",
    "        # Compute the next latent by comining with CLIP and downsampling\n",
    "        latent = self.clip_film(latent, clip)\n",
    "        latent = self.down(latent)\n",
    "\n",
    "        # Flatten the resulting latent and combine with the UNet\n",
    "        latent_flat = latent.view(latent.shape[0], -1)\n",
    "        unet = self.unet_film(unet, latent_flat)\n",
    "\n",
    "        # Return the UNet result and the latent for use by the next ReferenceNet block\n",
    "        return unet, latent\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# ref_block = ReferenceNetBlock(16, 32, 8, 8, 3, 512)\n",
    "\n",
    "# latent = torch.randn((1, 16, 16, 16))\n",
    "# clip_features = torch.randn((1, 512))\n",
    "# x_unet = torch.randn((1, 3, 128, 128))\n",
    "\n",
    "# x_unet, latent = ref_block(x_unet, latent, clip)\n",
    "# print(f\"x_unet: {x_unet.shape}, latent: {latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pose Guider\n",
    "\n",
    "Encodes the pose into some (??) feature space that is simply added\n",
    "to the noisy starting image.\n",
    "\n",
    "Simply a 4-layer conv with a sizable hidden dimension.\n",
    "\"\"\"\n",
    "\n",
    "class PoseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class PoseGuider(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            PoseBlock(in_channels, hidden_dims),\n",
    "            PoseBlock(hidden_dims, hidden_dims),\n",
    "            PoseBlock(hidden_dims, hidden_dims),\n",
    "            PoseBlock(hidden_dims, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Load the pose\n",
    "# image = load_png('assets/pose.png')\n",
    "# pose = image_to_tensor(image)\n",
    "\n",
    "# # Create a noisy initial image\n",
    "# noisy = torch.randn_like(pose)\n",
    "\n",
    "# # Call the model and add the encoded result to the noise\n",
    "# pose_model = PoseGuider(3, 3, 64)\n",
    "# y = pose_model(ref)\n",
    "# y += noisy\n",
    "# print(f\"UNet input shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14677fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Denoising UNet\n",
    "\"\"\"\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.GroupNorm(2, out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)  # downsample\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DenoisingUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Pose Encoder\n",
    "        self.pose_enc = PoseGuider(3, 3, 64)\n",
    "\n",
    "        # Reference Encoder\n",
    "        self.ref1 = ReferenceNetBlock(16, 32, 8, 8, 3, 512)\n",
    "        self.enc1 = ConvBlock(3, 16)\n",
    "\n",
    "        self.ref2 = ReferenceNetBlock(32, 64, 4, 4, 16, 512)\n",
    "        self.enc2 = DownBlock(16, 32)\n",
    "\n",
    "        self.ref3 = ReferenceNetBlock(64, 128, 2, 2, 32, 512)\n",
    "        self.enc3 = DownBlock(32, 64)\n",
    "\n",
    "        self.ref4 = ReferenceNetBlock(128, 256, 1, 1, 64, 512)\n",
    "        self.enc4 = DownBlock(64, 128)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(128, 128)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = UpBlock(128, 64)\n",
    "        self.dec3 = UpBlock(128, 32)\n",
    "        self.dec2 = UpBlock(64, 16)\n",
    "        self.dec1 = ConvBlock(32, 8)\n",
    "\n",
    "        self.out = nn.Conv2d(8, 3, kernel_size=1)\n",
    "    \n",
    "    def forward(self, noisy, pose, latent, clip):\n",
    "        # Encode\n",
    "        pose = self.pose_enc(pose) + noisy\n",
    "\n",
    "        tmp, latent = self.ref1(pose, latent, clip)\n",
    "        e1 = self.enc1(tmp)\n",
    "        \n",
    "        tmp, latent = self.ref2(e1, latent, clip)\n",
    "        e2 = self.enc2(tmp)\n",
    "\n",
    "        tmp, latent = self.ref3(e2, latent, clip)\n",
    "        e3 = self.enc3(tmp)\n",
    "\n",
    "        tmp, _ = self.ref4(e3, latent, clip)\n",
    "        e4 = self.enc4(tmp)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.dec4(b)\n",
    "        d3 = self.dec3(torch.cat([d4, e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([d3, e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([d2, e1], dim=1))\n",
    "\n",
    "        return self.out(d1)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# noise = torch.randn((1, 3, 128, 128))\n",
    "# latent = torch.randn((1, 16, 16, 16))\n",
    "# clip = torch.randn((1, 512))\n",
    "# unet = DenoisingUNet()\n",
    "# y = unet(noise, latent, clip)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiFileDataset(Dataset):\n",
    "    def __init__(self, file_paths: List):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_sizes = [len(torch.load(f, map_location='cpu')) for f in file_paths]\n",
    "        self._cum_file_sizes = [sum(self.file_sizes[:i+1]) for i in range(len(self.file_sizes))]\n",
    "\n",
    "        self._file_idx = None\n",
    "        self._data = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self.file_sizes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target_file_idx, data_idx = self._find_file(idx)\n",
    "        if target_file_idx != self._file_idx:\n",
    "            self._data = torch.load(self.file_paths[target_file_idx], map_location='cpu')\n",
    "        return self._data[data_idx]\n",
    "\n",
    "    def _find_file(self, idx):\n",
    "        for i, c in enumerate(self._cum_file_sizes):\n",
    "            if idx < c:\n",
    "                file_idx = i\n",
    "                data_idx = idx if i == 0 else idx - self._cum_file_sizes[i-1]\n",
    "                return file_idx, data_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc711e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Create the dataset\n",
    "file_paths = [str(p) for p in Path('trainset').glob('*.pt')]\n",
    "dataset = MultiFileDataset(file_paths)\n",
    "\n",
    "# Split\n",
    "train_split, val_split, test_split = 0.8, 0.1, 0.1\n",
    "n_total = len(dataset)\n",
    "n_train = int(train_split * n_total)\n",
    "n_val = int(val_split * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_set, val_set, test_set = random_split(\n",
    "    dataset,\n",
    "    [n_train, n_val, n_test]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4de42096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss = 0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss = 0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss = 0.0263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss = 0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss = 0.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss = 0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss = 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss = 0.0331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss = 0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss = 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss = 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss = 0.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss = 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss = 0.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss = 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[32m     19\u001b[39m     unet.train()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move to GPU\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get inputs\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TODO: look into this extra dim  issue. Shapes are 8, 1, 1, 3, 128, 128 for some reason\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/pixelpose/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/pixelpose/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/pixelpose/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/pixelpose/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/pixelpose/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1290\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/queue.py:213\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Basic setup\n",
    "N_EPOCHS = 10000\n",
    "device = torch.device('cuda')\n",
    "# unet = DenoisingUNet().to(device)\n",
    "# optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "\n",
    "# Noise params\n",
    "T = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    unet.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        # Move to GPU\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "        # Get inputs\n",
    "        # TODO: look into this extra dim  issue. Shapes are 8, 1, 1, 3, 128, 128 for some reason\n",
    "        latent = batch['reference_vae'].squeeze(1).squeeze(1)\n",
    "        clip = batch['reference_clip'].squeeze(1).squeeze(1)\n",
    "        pose = batch['pose'].squeeze(1).squeeze(1)\n",
    "\n",
    "        # Prepare the target\n",
    "        target = batch['target'].squeeze(1).squeeze(1)\n",
    "        B = target.size(0)\n",
    "        t = torch.randint(0, T, (B,), device=device)\n",
    "        eps = torch.randn_like(target)\n",
    "        a_bar_t = alphas_cumprod[t].view(B, 1, 1, 1)\n",
    "        noisy = torch.sqrt(a_bar_t) * target + torch.sqrt(1 - a_bar_t) * eps\n",
    "\n",
    "        # Forward\n",
    "        pred_eps = unet(noisy, pose, latent, clip)\n",
    "\n",
    "        # Loss\n",
    "        loss = F.mse_loss(pred_eps, eps)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcd1f78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36390936374664307,\n",
       " 0.167206808924675,\n",
       " 0.13038450479507446,\n",
       " 0.10169705003499985,\n",
       " 0.12740829586982727,\n",
       " 0.08056183904409409,\n",
       " 0.10691474378108978,\n",
       " 0.10771314799785614,\n",
       " 0.11733394861221313,\n",
       " 0.1254330575466156,\n",
       " 0.1074734628200531,\n",
       " 0.08809306472539902,\n",
       " 0.09401794523000717,\n",
       " 0.10569876432418823,\n",
       " 0.11191048473119736,\n",
       " 0.07192958891391754,\n",
       " 0.09839329123497009,\n",
       " 0.09563539922237396,\n",
       " 0.095150887966156,\n",
       " 0.12466983497142792,\n",
       " 0.10174554586410522,\n",
       " 0.0638820081949234,\n",
       " 0.06841405481100082,\n",
       " 0.05968412384390831,\n",
       " 0.057460665702819824,\n",
       " 0.061801727861166,\n",
       " 0.048900969326496124,\n",
       " 0.07124795019626617,\n",
       " 0.07378151267766953,\n",
       " 0.055166613310575485,\n",
       " 0.08690949529409409,\n",
       " 0.07315545529127121,\n",
       " 0.06633316725492477,\n",
       " 0.042446475476026535,\n",
       " 0.07170681655406952,\n",
       " 0.038401179015636444,\n",
       " 0.03812115639448166,\n",
       " 0.04186611622571945,\n",
       " 0.05014296621084213,\n",
       " 0.06284771859645844,\n",
       " 0.05711023509502411,\n",
       " 0.07623037695884705,\n",
       " 0.040004950016736984,\n",
       " 0.05278850719332695,\n",
       " 0.026685133576393127,\n",
       " 0.04152636229991913,\n",
       " 0.05242582783102989,\n",
       " 0.0466335229575634,\n",
       " 0.04566469416022301,\n",
       " 0.043267540633678436,\n",
       " 0.039780743420124054,\n",
       " 0.05143391713500023,\n",
       " 0.048404864966869354,\n",
       " 0.051874421536922455,\n",
       " 0.04576709121465683,\n",
       " 0.05407993122935295,\n",
       " 0.03187968581914902,\n",
       " 0.04250922054052353,\n",
       " 0.06285659968852997,\n",
       " 0.03526530787348747,\n",
       " 0.04887871444225311,\n",
       " 0.039403706789016724,\n",
       " 0.05205094814300537]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5647b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 2000it [00:02, 699.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(unet, pose, latent, clip, T=2000, device=\"cuda\"):\n",
    "    unet.eval()\n",
    "\n",
    "    # Diffusion schedule (must match training)\n",
    "    betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), alphas_cumprod[:-1]])\n",
    "\n",
    "    # Start from pure noise (same shape as your target)\n",
    "    x_t = torch.randn((1, 3, 128, 128), device=device)\n",
    "\n",
    "    for t in tqdm(reversed(range(T)), desc=\"Sampling\"):\n",
    "        # Prepare t tensor\n",
    "        t_batch = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict the noise at this step\n",
    "        eps_pred = unet(x_t, pose, latent, clip)\n",
    "\n",
    "        # Compute posterior mean (DDPM update)\n",
    "        beta_t = betas[t]\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alphas_cumprod[t]\n",
    "        alpha_bar_prev = alphas_cumprod_prev[t]\n",
    "\n",
    "        # Equation from DDPM paper\n",
    "        pred_x0 = (x_t - torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t)\n",
    "        coef1 = torch.sqrt(alpha_bar_prev) * beta_t / (1 - alpha_bar_t)\n",
    "        coef2 = torch.sqrt(alpha_t) * (1 - alpha_bar_prev) / (1 - alpha_bar_t)\n",
    "        mean = coef1 * pred_x0 + coef2 * x_t\n",
    "\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x_t = mean + sigma_t * noise\n",
    "        else:\n",
    "            x_t = mean\n",
    "\n",
    "    # Optional: decode with VAE\n",
    "    img = x_t\n",
    "\n",
    "    # Map [-1,1] â†’ [0,1] for display\n",
    "    img = (img.clamp(-1, 1) + 1) / 2\n",
    "    return img\n",
    "\n",
    "\n",
    "idx = random.randint(0, len(dataset) - 1)\n",
    "sample = train_set[idx]\n",
    "\n",
    "latent = sample['reference_vae'].squeeze(0).to(device)\n",
    "clip = sample['reference_clip'].squeeze(0).to(device)\n",
    "pose = sample['pose'].squeeze(0).to(device)\n",
    "\n",
    "image = generate(unet, pose, latent, clip)\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "img_tensor = image.squeeze(0).cpu()\n",
    "img_pil = to_pil_image(img_tensor)\n",
    "img_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a14f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state\": unet.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict()\n",
    "}, \"checkpoints/unet_checkpoint.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc58e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fe072",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a37b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89232d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoints/unet_checkpoint.pt\", map_location=device)\n",
    "unet.load_state_dict(checkpoint[\"model_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "start_epoch = checkpoint[\"epoch\"] + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
